# Consumer Complaints Analysis (NLP & Topic Modeling)

Dieses Repository enth√§lt die Codebasis f√ºr das Portfolio im Bereich Data Science / Data Analysis. 
Ziel des Projekts ist der Vergleich verschiedener NLP-Verfahren zur Vektorisierung und Themenmodellierung sowie die Entwicklung einer optimierten Pipeline zur Verarbeitung unstrukturierter Textdaten aus dem "Consumer Complaints" Datensatz.
Dabei sollen automatisch die Hauptbeschwerdegr√ºnde der enthaltenen Kundenbeschwerden identifiziert werden.

## Projekt√ºbersicht

Das Projekt vergleicht verschiedene Vektorisierungs- und Modellierungsans√§tze, um eine robuste Pipeline f√ºr Kurztexte zu entwickeln:

* **Datenbasis:** Consumer Complaints Dataset (Finanzbeschwerden)
* **Vektorisierung:** Vergleich von **TF-IDF** (statistisch) und **Word2Vec** (semantisch)
* **Topic Modeling:** Vergleich von **Latent Dirichlet Allocation (LDA)** und **Non-negative Matrix Factorization (NMF)**

## Ergebnisse

F√ºr die Reproduzierbarkeit wurde ein Seed eingef√ºgt, um sicherzustellen, dass bei der Anwendung von Grid-Search immer die gleichen Daten geladen werden. 
Die Analyse zeigte signifikante Unterschiede zwischen den Verfahren:

1.  **Identifizierte Pipeline (TF-IDF & NMF):** Die Kombination aus **TF-IDF** und **NMF** erwies sich als einziger robuster Ansatz. W√§hrend Word2Vec keine koh√§renten Cluster f√ºr die Themenmodellierung lieferte und LDA zu thematischem Rauschen neigte, erzeugte NMF die mit Abstand trennsch√§rfsten Ergebnisse.2.  **Optimale Themenzahl:** Durch Koh√§renz-Analysen auf Teildatens√§tzen (20-80%) wurde **$K=5$** als optimales Cluster-Setup ermittelt.
3.  **Identifizierte Themen:**
    * Inkasso & Schulden (*Debt Collection*)
    * Credit Reporting (*Fehlerhafte Eintr√§ge*)
    * Kredite & Hypotheken (*Loans/Mortgages*)
    * Identit√§tsdiebstahl (*Theft/Fraud*)
    * Rechtliche Beschwerden (*Legal/Regulatory*)

## üõ† Installation

Dieses Projekt wurde mit **Python 3.11.9** entwickelt. 
Um Kompatibilit√§tsprobleme zu vermeiden, wird die Verwendung von Linux in Kobination mit `pyenv` zur Verwaltung der Python-Version empfohlen.

1. **Virtuelle Umgebung installieren (empfohlen)**

Falls `pyenv` noch nicht installiert ist, folge bitte dieser Anleitung:

<details>
<summary><strong>Linux</strong></summary>
```bash
curl https://pyenv.run | bash
```
Folgen Sie den Bildschirmanweisungen, um pyenv zur Shell hinzuzuf√ºgen.
</details>

2. **Repository klonen:**

    ```bash
    git clone https://github.com/StaticFrost-No1/Project-Data-Analysis.git
    cd Project-Data-Analysis
    ```

3. **Python 3.11.9 installieren**

    ```bash
    pyenv install 3.11.9
    pyenv local 3.11.9
    ```

4. **Virtuelle Umgebung erstellen und aktivieren (empfohlen):**

    ```bash
    pyenv exec python -m venv .venv
    source .venv/bin/activate
    ```

5. **Abh√§ngigkeiten installieren:**

    ```bash
    pip install -r requirements.txt
    ```

## Nutzung 

Der Code wurde f√ºr die kontrollierte Verwendung in vier Abschnitte unterteilt.
Die Pipeline sollte von Schritt 1-4 nacheinander ausgef√ºhrt werden. Ergebnisse k√∂nnen direkt verglichen werden
Phase 3 und 4 sind sehr rechenintensiv und enthalten Parameter, die eine manuelle Anpassung der Balance zwischen Rechenzeit und Pr√§zision 
an die eigenen Bed√ºrfnisse erm√∂glichen.

### Pipeline

Die Skripte sollten idealerweise in folgender Reihenfolge ausgef√ºhrt werden:

1.  **Preprocessing:** Bereinigung der Rohdaten.
    ```bash
    python 1_preprocessing.py
    ```
2.  **Vektorisierung:** Erstellung der TF-IDF und Word2Vec Modelle.
    ```bash
    python 2_vectorization.py
    ```
3.  **Koh√§renz-Berechnung:** Suche nach dem optimalen $K$.
    ```bash
    python 3_optimization_coherence.py
    ```
4.  **Themen-Modellierung:** Generierung der Themen mit NMF.
    ```bash
    python 4_final_topic_modeling.py
    ```

### Parameter

die eine manuelle anpassung der Gr√∂√üe der verarbeiteten Teildatens√§tze, verwendeten CPU-Kerne, 
und Anzahl der Durchl√§ufe erm√∂glichen, 

- **SAMPLE_FRAC:** Sampling-Rate passt die Gr√∂√üe des verarbeiteten Teildatensatzes f√ºr an (0.60 = 60% der Daten)
    - Ein h√∂herer Wert steigert zwar die Pr√§zision, erh√∂ht aber Rechenzeit und Speicherbedarf signifikant
    - St√ºrzt das Programm ab, sollte dieser Wert nach unten angepasst werden.
    - Ein Wert von `0.60` hat sich als idealer Kompromiss zwischen Pr√§zision und Ressourcenverbrauch erwiesen
- **NUM_TOPICS:** Empfohlene Themenzahl K (nur bei der Themen-Modellierung)
- **WORKERS:** Zahl der verwendeten CPU-Kerne (mehr Kerne ben√∂tigen mehr RAM)
- **PASSES:** Anzahl der kompletten Durchl√§ufe (empfohlen: 10)

## Lizenz

Dieses Projekt ist unter der [MIT License](LICENSE) lizenziert.